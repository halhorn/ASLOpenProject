{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/ASLOpenProject/youtube8m'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# youtube8m のディレクトリに移動\n",
    "import os\n",
    "while os.getcwd().split('/')[-1] != 'youtube8m': os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather([5, 6, 7, 8, 9], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video.data import id_category_id_table\n",
    "print(id_category_id_table[:10])\n",
    "t, _ = tf.unique(tf.gather(id_category_id_table, [1, 2, 3, 0]))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(t.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([[1], [2], [3], [4]])\n",
    "m = 3\n",
    "d = m - t.shape[0]\n",
    "p = max(d, 0)\n",
    "print(p)\n",
    "tf.pad(t[:m,:], [[p, 0], [0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "CLASS_NUM = 3862\n",
    "RGB_DIM = 1024\n",
    "AUDIO_DIM = 128\n",
    "MAX_LEN = 300\n",
    "\n",
    "def multi_hot(indices):\n",
    "    return tf.reduce_sum(tf.one_hot(indices, CLASS_NUM), axis=-2)\n",
    "\n",
    "def dequantize(feat_vector, max_quantized_value=2, min_quantized_value=-2):\n",
    "    '''\n",
    "    8bit に圧縮されているデータを float32 に戻します\n",
    "    see: https://github.com/linrongc/youtube-8m/blob/master/utils.py#L28\n",
    "    '''\n",
    "    feat_vector = tf.cast(feat_vector, tf.float32)\n",
    "    quantized_range = max_quantized_value - min_quantized_value\n",
    "    scalar = quantized_range / 255.0\n",
    "    bias = (quantized_range / 512.0) + min_quantized_value\n",
    "    return feat_vector * scalar + bias\n",
    "\n",
    "def adjust_length(feature):\n",
    "    f = feature[:MAX_LEN,:]\n",
    "    l = tf.unstack(tf.shape(f))[0]\n",
    "    return tf.cond(\n",
    "        l < MAX_LEN,\n",
    "        lambda: tf.pad(f, [[MAX_LEN - l, 0], [0, 0]]),\n",
    "        lambda: f\n",
    "    )\n",
    "\n",
    "def decode(feature, dim):\n",
    "    '''\n",
    "    バイト列になっているフィーチャーを float32 の配列にして返します。\n",
    "    '''\n",
    "    f = tf.reshape(\n",
    "        tf.decode_raw(feature, tf.uint8),\n",
    "        [-1, dim],  # [len, dim]\n",
    "    )\n",
    "    f = adjust_length(f)\n",
    "    f = dequantize(f)\n",
    "    return f\n",
    "\n",
    "def parse_row(row):\n",
    "    context_features = {\n",
    "        \"id\": tf.FixedLenFeature([], tf.string),\n",
    "        \"labels\": tf.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    sequence_features = {\n",
    "        \"rgb\": tf.io.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "        \"audio\": tf.io.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "    }\n",
    "    context_data, sequence_data = tf.parse_single_sequence_example(row, context_features, sequence_features)\n",
    "    label = multi_hot(tf.sparse.to_dense(context_data['labels']))\n",
    "    label.set_shape([CLASS_NUM])\n",
    "    rgb = decode(sequence_data['rgb'], RGB_DIM)\n",
    "    audio = decode(sequence_data['audio'], AUDIO_DIM)\n",
    "    features = {\n",
    "        'id': context_data['id'],\n",
    "        'rgb': rgb,\n",
    "        'audio': audio,\n",
    "    }\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def read_dataset(files_pattern, mode, batch_size=128):\n",
    "    tffiles = tf.io.gfile.glob(files_pattern)\n",
    "    dataset = tf.data.TFRecordDataset(tffiles)\n",
    "    dataset = dataset.map(\n",
    "        parse_row,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(batch_size*10).repeat().batch(batch_size)\n",
    "    else:\n",
    "        dataset = dataset.repeat(1).batch(batch_size)\n",
    "    return dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = read_dataset(\n",
    "    'gs://asl-mixi-project-bucket/data/youtube-8m-frame/train/train0000.tfrecord',\n",
    "    tf.estimator.ModeKeys.TRAIN,\n",
    "    batch_size=3,\n",
    ")\n",
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from frame.data import adjust_length, dequantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_length(tf.constant([[1], [2], [3]]), max_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_length(tf.constant([[[1], [2], [3]]]), max_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.4235599 ]\n",
      " [-0.39218736]\n",
      " [-0.37650108]], shape=(5, 1), dtype=float32) tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "f = tf.constant([[100], [102], [103]], dtype=tf.uint8)\n",
    "dim = 1\n",
    "max_len = 5\n",
    "f = dequantize(f)\n",
    "f, length = adjust_length(f, max_len)\n",
    "f.set_shape([max_len, dim])\n",
    "print(f, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "with tf.Graph().as_default():\n",
    "    c = tf.constant([1] * 100)\n",
    "    p = tf.print(c, summarize=-1)\n",
    "    with tf.control_dependencies([p]):\n",
    "        c = tf.identity(c)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Dropout()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frame.attention.common_layer import FeedForwardNetwork, ResidualNormalizationWrapper, LayerNormalization\n",
    "from frame.attention.embedding import AddPositionalEncoding\n",
    "from frame.attention.attention import SelfAttention\n",
    "\n",
    "\n",
    "class AttentionModel(tf.keras.models.Model):\n",
    "    def __init__(self, params, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        hopping_num = params.get('hopping_num', 6)\n",
    "        head_num = params.get('head_num', 8)\n",
    "        hidden_dim = (params.get('hidden_dim', 512) // head_num) * head_num\n",
    "        dropout_rate = params.get('dropout', 0.1)\n",
    "        \n",
    "        self.input_dense = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.add_position_embedding = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.attention_block_list = []\n",
    "        for _ in range(hopping_num):\n",
    "            attention_layer = SelfAttention(hidden_dim, head_num, dropout_rate, name='self_attention')\n",
    "            ffn_layer = FeedForwardNetwork(hidden_dim, dropout_rate, name='ffn')\n",
    "            self.attention_block_list.append([\n",
    "                ResidualNormalizationWrapper(attention_layer, dropout_rate, name='self_attention_wrapper'),\n",
    "                ResidualNormalizationWrapper(ffn_layer, dropout_rate, name='ffn_wrapper'),\n",
    "            ])\n",
    "        self.output_normalization = LayerNormalization()\n",
    "        self.output_layer = tf.keras.layers.Dense(params['output_dim'])\n",
    "\n",
    "    def call(\n",
    "            self,\n",
    "            visual_feature,\n",
    "            audio_feature,\n",
    "            training=None,\n",
    "    ):\n",
    "        '''\n",
    "        モデルを実行します\n",
    "\n",
    "        :param visual_feature: shape = [batch_size, length, dim]\n",
    "        :param audio_feature: shape = [batch_size, length, dim]\n",
    "        :param training: 学習時は True\n",
    "        :return: shape = [batch_size, output_dim]\n",
    "        '''\n",
    "        input = tf.concat([visual_feature, audio_feature], axis=-1)\n",
    "        input = self.input_dense(input)\n",
    "        tf.print('input', input.shape)\n",
    "        self_attention_mask = self._create_enc_attention_mask(input)\n",
    "        embedded_input = self.add_position_embedding(input)\n",
    "        tf.print('emb', embedded_input)\n",
    "        query = self.input_dropout_layer(embedded_input, training=training)\n",
    "        tf.print(query.shape)\n",
    "\n",
    "        for i, layers in enumerate(self.attention_block_list):\n",
    "            attention_layer, ffn_layer = tuple(layers)\n",
    "            with tf.name_scope('hopping_{}'.format(i)):\n",
    "                query = attention_layer(query, attention_mask=self_attention_mask, training=training)\n",
    "                query = ffn_layer(query, training=training)\n",
    "        # [batch_size, length, hidden_dim]\n",
    "        attention_output = self.output_normalization(query)\n",
    "        return self.output_layer(attention_output[:,0,:])\n",
    "\n",
    "    def _create_enc_attention_mask(self, encoder_input: tf.Tensor):\n",
    "        with tf.name_scope('enc_attention_mask'):\n",
    "            encoder_input = tf.reduce_sum(encoder_input, axis=-1)  # [batch_size, length]\n",
    "            batch_size, length = tf.unstack(tf.shape(encoder_input))\n",
    "            pad_array = tf.equal(encoder_input, 0.0)  # [batch_size, m_length]\n",
    "            # shape broadcasting で [batch_size, head_num, (m|q)_length, m_length] になる\n",
    "            return tf.reshape(pad_array, [batch_size, 1, 1, length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttentionModel({'output_dim': 12})(tf.ones([4, 3, 2]), tf.ones([4, 3, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "categories = {}\n",
    "with open('./video/vocabulary.csv', 'r') as c:\n",
    "    reader = csv.reader(c)\n",
    "    print(next(reader))\n",
    "    for r in reader:\n",
    "        category = r[5]\n",
    "        if category not in categories:\n",
    "            categories[category] = 0\n",
    "        categories[category] += 1\n",
    "sorted(categories.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'aa,bb,cc\\ndd,ee,cc\\n'\n",
    "import io\n",
    "with io.StringIO(text) as f:\n",
    "    print(next(csv.reader(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "categories = {}\n",
    "with open('./video/vocabulary.csv', 'r') as c:\n",
    "    reader = csv.reader(c)\n",
    "    next(reader)\n",
    "    print(list(reader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "class_num = 8\n",
    "category_num = 3\n",
    "weight = tf.expand_dims(tf.constant([1.0] * class_num + [2.0] * category_num), axis=0)\n",
    "t = tf.ones([batch_size, class_num + category_num])\n",
    "t * weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights[:class_num] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.metrics.accuracy(labels=[1.0, 2.0, 3.0], predictions=[1.0, 2.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
